name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: 3.12
  NODE_VERSION: 18

jobs:
  # Code Quality Checks
  lint-and-format:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install black isort flake8 mypy bandit safety
          pip install -r requirements-test.txt

      - name: Run Black (code formatting)
        run: black --check --diff src/ tests/

      - name: Run isort (import sorting)
        run: isort --check-only --diff src/ tests/

      - name: Run flake8 (linting)
        run: flake8 src/ tests/ --max-line-length=88 --extend-ignore=E203,W503

      - name: Run mypy (type checking)
        run: mypy src/ --ignore-missing-imports

      - name: Run bandit (security linting)
        run: bandit -r src/ -f json -o bandit-report.json || true

      - name: Run safety (dependency security check)
        run: safety check --json --output safety-report.json || true

      - name: Upload security reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json

  # Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.10, 3.11, 3.12]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py${{ matrix.python-version }}-pip-${{ hashFiles('**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Run unit tests
        run: |
          pytest tests/unit/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-fail-under=80 \
            --junitxml=test-results-unit.xml \
            -v
        env:
          TESTING: "1"

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results-py${{ matrix.python-version }}
          path: |
            test-results-unit.xml
            htmlcov/
            .coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unit-tests
          name: codecov-py${{ matrix.python-version }}

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_chatbot
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Set up test database
        run: |
          export DATABASE_URL="postgresql://test_user:test_password@localhost:5432/test_chatbot"
          # Run database migrations if they exist
          # alembic upgrade head

      - name: Run integration tests
        run: |
          pytest tests/integration/ \
            --cov=src \
            --cov-append \
            --cov-report=xml \
            --junitxml=test-results-integration.xml \
            -v
        env:
          TESTING: "1"
          DATABASE_URL: "postgresql://test_user:test_password@localhost:5432/test_chatbot"
          OPENAI_API_KEY: "test-key-not-real"
          GITHUB_TOKEN: "test-token-not-real"

      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: |
            test-results-integration.xml
            .coverage

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[perf]')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Run performance tests
        run: |
          pytest tests/performance/ \
            --benchmark-json=benchmark-results.json \
            --junitxml=test-results-performance.xml \
            -v
        env:
          TESTING: "1"

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-test-results
          path: |
            benchmark-results.json
            test-results-performance.xml

      - name: Performance regression check
        run: |
          # Compare with baseline performance metrics
          # Fail if performance degrades significantly
          python scripts/check_performance_regression.py benchmark-results.json

  # Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Run security tests
        run: |
          pytest tests/ -m security \
            --junitxml=test-results-security.xml \
            -v
        env:
          TESTING: "1"

      - name: Upload security test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-test-results
          path: test-results-security.xml

  # Build and Package
  build:
    name: Build Application
    runs-on: ubuntu-latest
    needs: [lint-and-format, unit-tests]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install build dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build wheel setuptools

      - name: Build package
        run: |
          python -m build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v3
        with:
          name: python-package
          path: dist/

  # Docker Build
  docker-build:
    name: Docker Build
    runs-on: ubuntu-latest
    needs: [build]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: whatsapp-ai-chatbot
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # Deployment (Production)
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, docker-build]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to production
        run: |
          echo "Deploying to production environment..."
          # Add deployment scripts here
          # Examples:
          # - Deploy to cloud provider (AWS, GCP, Azure)
          # - Update Kubernetes deployments
          # - Run database migrations
          # - Update load balancer configurations

      - name: Run smoke tests
        run: |
          echo "Running smoke tests on production..."
          # Add smoke tests to verify deployment
          # curl -f https://your-app.com/health || exit 1

      - name: Notify deployment status
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          message: |
            Deployment to production: ${{ job.status }}
            Commit: ${{ github.sha }}
            Author: ${{ github.actor }}

  # Cleanup
  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: always()
    steps:
      - name: Clean up test data
        run: |
          echo "Cleaning up test resources..."
          # Clean up any test databases, temporary files, etc.

  # Nightly Comprehensive Tests
  nightly-tests:
    name: Nightly Comprehensive Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    strategy:
      matrix:
        test-suite: [unit, integration, performance, security]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Run comprehensive test suite
        run: |
          pytest tests/${{ matrix.test-suite }}/ \
            --cov=src \
            --cov-report=xml \
            --junitxml=test-results-nightly-${{ matrix.test-suite }}.xml \
            --maxfail=5 \
            -v
        env:
          TESTING: "1"
          COMPREHENSIVE_TESTS: "1"

      - name: Upload nightly test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: nightly-test-results-${{ matrix.test-suite }}
          path: |
            test-results-nightly-${{ matrix.test-suite }}.xml
            .coverage

      - name: Report test failures
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          message: |
            Nightly ${{ matrix.test-suite }} tests failed!
            Check the logs: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

  # Generate Test Report
  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v3

      - name: Generate comprehensive test report
        run: |
          python scripts/generate_test_report.py \
            --unit-results unit-test-results*/test-results-unit.xml \
            --integration-results integration-test-results/test-results-integration.xml \
            --output test-report.html

      - name: Upload test report
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-test-report
          path: test-report.html

      - name: Comment test results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('test-report.html')) {
              const report = fs.readFileSync('test-report.html', 'utf8');
              // Extract summary from report and post as comment
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: 'Test results summary:\n\nSee artifacts for detailed report.'
              });
            }